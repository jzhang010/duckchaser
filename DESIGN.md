# Computer Vision in Robotics - Design Document
## Project Organization
The project struture borrows heavily from the FTC robotics library (https://github.com/FIRST-Tech-Challenge/FtcRobotController), which includes all of the hardware libraries and the source code used to build an Android app to control a robot. We had to learn how to work inside of the project template provided and write all of the different subsystem classes that defines the functionality of the different hardware components on the robot and also robot program classes that the user actually runs on the robot. All our original code can be found by going to duckchaser/TeamCode/src/main/java/org/firstinspires/ftc/teamcode. The teamcode is organized into two different folders, operation and subsystems, as we wanted thought it made sense logically to write all of the functionalities of the different subsystems and use those functionalities in different robot operation programs. 

## Subsystems 
### MecanumDriveTrain 
This class defines all of the functionality of the mecanum drivetrain, which is what the robot uses to move around. To implement a this class, we needed to program the four motors that power the entire drivetrain. The FTC library already come with methods that allow us to move the motors, so we just needed to program the logic in making the motors spin in the correct direction for a given desired action, such as driving forwards or stopping. Following object oriented programming, we defined all of the different actions the drivetrain can do as methods.
#### Driving, Turning, and Strafing
After researching how a drivetrain should work, we found that if we want to move the robot forward, we simply needed to spin all of the motors in the foward direction. Likewise, if we want to robot to move backwards, we needed to spin all of the motors in the backwards direction. If we want the robot to turn to the left, we need to make the right side motors spin faster forward than the left side motors, and the opposite is true for turning to the left. We found that if we want to robot to turn about its center, we make the motors one side go forward at a given power and the other side go backwards at the same power, and we liked that turning behavior. Now since the robot has a mecanum drivetrain, we can also program it to strafe right and left, which means that the entire drivetrain can move sideways because of the angled rollers on the wheels. We found that the two sets of diagonal wheels in opposite direction from the other set will make the robot strafe. All of this logic is captured in the drive method, and the humanControl method maps the gamped joysticks to the different drivetrain action. 
#### Autonomous Driving
For autonomous driving, we wanted the robot to drive forward or strafe sidewyas by a certain number of inches because we thought it would be more accurate than have the robot drive for a certain amount of time, especially when the surface that the robot is driven can vary. All four motors on the robot have encoders, which measures the rotation of the motors. However, encoders return the number of ticks the motor has rotated through, which is not a very helpful unit of measurement when we want the robot to travel a certain distance. By looking up the motor specification, we can find the number of ticks per revolution. By measuring the wheel diameter and calculating the circumference of the wheel, we can find the number of ticks per inch, which is allows us to write methods that drive the robot a certain number of inches. The FTC library includes a RUN_TO_POSITION mode, which uses a PID control to turn all motors to a target position. We used this PID control mode because a PID control mode will use constnatly use 3 different feedback factors to adjust the motor powers to make the robot drive smoothly toward the target. For example, P stands for proportional, which in this case, means that the robot should slow down when it is approaching the target position in order to not overshoot it. The class also has a lot of useful helper functions, such as a function that simply stops the robot from moving and a function that logs the current motor power values. 

### EyeOpenCV and EyePipeline 
The classes borrow heavily from the FTC Open CV library: https://github.com/OpenFTC/EasyOpenCV. We ultimately couldn't integrate our machine learning model for finding the duck with the robot code in time, so we opted to work off of a library that can be easily integrated with FTC code. We ended up splitting the subsystem into two classes as it was getting too long by itself, as the EyeOpenCV class is the logic for opening the camera stream and the EyePipeline includes all of the logic of finding the duck using computer vision. The basic logic for finding the duck is that the pipeline looks at 3 different predefined regions in the camera, which correspond to a left, center, or right position. We only look at 3 regions because it makes our code simple and logic much easier to write. The code compares the average YCrCb pixel values in a region. In YCbCr, the Y is the brightness (luma), Cb is blue minus luma (B-Y) and Cr is red minus luma (R-Y). Thus, unlike RGB values, YCbCr values will not be severely affected by differing light intensity, since that difference would most likely just be reflected in the Y channel. We specifically want to look at the Cb channel because the duck is bright yellow and contrasts strongly on the Cb channel against everything else. The brightest of the 3 regions and if it crosses a certain brightness threshold is where we assume the duck to be. The pipeline class also includes code for visual aids when using the camera for duck detection. 

### Waver
The Waver subsystem is implemented by just programming the one servo that makes up the waving "tail" at the back of the robot, and we can set the position of the servo using methods in the FTC library. Mirroring the 3 positions we defined in EyeOpenCV, we define a left, center, and right position for the waver as methods we can use in other classes to set the waver to. We also thought it would be nice to add a waving method, which just waves the tail back and forth like an excited dog. 

## Operations
### Autonomous Drive to Duck 
This program autonomously drives the robot to the duck. We create a drivetrain, eye, and waver object in the program so we can utilize all of the functionality we wrote in the subsystem classes. The FTC library already gives us the framework for the autonomous opertation modes, so we just have to code what happens in each of the op mode states, specifically what happens when the user initializes the program and also when the user presses play to start the program. We want during the initialization of the program for the user to be able to see where the robot think the duck is and be able to adjust the camera accordingly, so we log that to telementry. When the program is started, we utilize the driving methods and the last recorded position of the duck to drove towards it. We also used the waver's waving method at the end to signal the end of the program. 

### Human Drive
This program allows a human to drive the robot. We create a drivetrain and waver object in the program so we can utilize all of the functionality we wrote in the subsystem classes. Again, the FTC library already gives us the framework for a teleoperated mode, which we will use for remote human operation of the robot through a gamepad controller. All we need to do for this class is to code what should happen when the program is initialized and also when the program is started. We specifically coded in the loop method since the action of taking human gamepad input and manuevering the robot should be constantly happening while the program is running. 

## Future Directions
### Duck_Detection_Model
Another possibility for duck identification involves training an object classifier in Tensorflow. Ideally, we would create and train a classifier from scratch, but there are not a lot of easily accessible image databases containing rubber duck pictures on the Internet. To get around this problem, we started by using VGG16. VGG16 is one of six models for VGG. VGG was created by researchers at Oxford University as a proposed architecture for very deep convolutional networks in response to the ImageNet Challenge, which asks researchers to classify objects into over one thousand objects based on millions of images. VGG16 allows programmers to easily identify new objects without having to create a classifier from scratch (which would take hours or days to train) and is especially suited to situations with imbalanced datasets or sparse data. By using a pre-trained VGG network, we can access patterns or weights that VGG found particularly useful in classifying objects and apply it to new objects that it was not trained for. This is called transfer learning, and it can be used for one-class classification (essentially, training datasets only have one class). In Tensorflow, we created a small training dataset containing five duck images and imported VGG16. We removed the last classification layer that used the ImageNet dataset from VGG16 and added two dense layers of our own to train the classifier on our duck images. The dense layers do not have many nodes because the training dataset was small. We then tested our trained classifier on a testing dataset comprised of duck and non-duck images. Unfortunately, we were not able to complete the classifier and connect to the robot in the time we were given. This is because our code was written in Python, which is very intuitive and easy for machine learning, and the robot's code was written in Java. If given more time, we would like to find a way to interface the two. The beginning of this code is contained in duck_detection_model.py.
